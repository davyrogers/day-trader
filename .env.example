# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# AI Models (add more for diverse perspectives!)
AI_MODELS=deepseek-r1:8b,gpt-oss:20b,gpt-oss:20b,gpt-oss:20b,llama3:70b,mistral:latest

# Model Temperatures (comma-separated, same order as models)
# Higher temp = more creative/diverse, Lower = more focused/consistent
AI_TEMPERATURES=0.7,0.8,0.9,1.0,0.85,0.75

# Synthesis Model (for final summary)
SYNTHESIS_MODEL=gpt-oss:20b
SYNTHESIS_TEMPERATURE=0.7

# Concurrent Execution (set to false if Ollama can't handle parallel requests)
RUN_CONCURRENT=false

# Discord Configuration
DISCORD_WEBHOOK_URL=your_webhook_url_here

# Workflow Configuration
RUN_ONCE=true
SCHEDULE_INTERVAL_HOURS=1

# Logging
LOG_LEVEL=INFO
